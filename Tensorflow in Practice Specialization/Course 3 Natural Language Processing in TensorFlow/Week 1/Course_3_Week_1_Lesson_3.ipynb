{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkaBMeNDwMel"
   },
   "source": [
    "# Kaggle: Sarcasm Internet Article NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json'\n",
    "wget.download(url)\n",
    "\n",
    "print('Finished download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 append text from jason into list\n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "# open json dictionary and append to three lists\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of word index:\n",
      " 29657\n",
      "\n",
      "Word index (first 100):\n",
      " {'<OOV>': 1, 'to': 2, 'of': 3, 'the': 4, 'in': 5, 'for': 6, 'a': 7, 'on': 8, 'and': 9, 'with': 10, 'is': 11, 'new': 12, 'trump': 13, 'man': 14, 'from': 15, 'at': 16, 'about': 17, 'you': 18, 'this': 19, 'by': 20, 'after': 21, 'up': 22, 'out': 23, 'be': 24, 'how': 25, 'as': 26, 'it': 27, 'that': 28, 'not': 29, 'are': 30, 'your': 31, 'his': 32, 'what': 33, 'he': 34, 'all': 35, 'just': 36, 'who': 37, 'has': 38, 'will': 39, 'more': 40, 'one': 41, 'into': 42, 'report': 43, 'year': 44, 'why': 45, 'have': 46, 'area': 47, 'over': 48, 'donald': 49, 'u': 50, 'day': 51, 'says': 52, 's': 53, 'can': 54, 'first': 55, 'woman': 56, 'time': 57, 'like': 58, 'her': 59, \"trump's\": 60, 'old': 61, 'no': 62, 'get': 63, 'off': 64, 'an': 65, 'life': 66, 'people': 67, 'obama': 68, 'now': 69, 'house': 70, 'still': 71, \"'\": 72, 'women': 73, 'make': 74, 'was': 75, 'than': 76, 'white': 77, 'back': 78, 'my': 79, 'i': 80, 'clinton': 81, 'down': 82, 'if': 83, '5': 84, 'when': 85, 'world': 86, 'could': 87, 'we': 88, 'their': 89, 'before': 90, 'americans': 91, 'way': 92, 'do': 93, 'family': 94, 'most': 95, 'gop': 96, 'they': 97, 'study': 98, 'school': 99, \"it's\": 100}\n",
      "\n",
      "First padded sequence:\n",
      " [  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "Size padded:\n",
      "Sentences: 26709, Longest sentence: 40\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 2 instantiate tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# Step 3 fit/train tokenizer on text\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Step 4 create word index/dictionary\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\nLength of word index:\\n\",len(word_index))\n",
    "import itertools\n",
    "print(\"\\nWord index (first 100):\\n\",dict(itertools.islice(word_index.items(), 100)))\n",
    "\n",
    "# Step 5 translate sentences into (different length) token sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Step 6 pad the token sequences to equalize length\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(\"\\nFirst padded sequence:\\n\",padded[0])\n",
    "print(\"\\nSize padded:\\nSentences: {}, Longest sentence: {}\".format(padded.shape[0], padded.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course 3 - Week 1 - Lesson 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
